{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Universidad de Buenos Aires\n",
        "# Aprendizaje Profundo - TP2\n",
        "# Cohorte 20 - 3er bimestre 2025\n"
      ],
      "metadata": {
        "id": "tHbzg4F1fLo7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este segundo TP se debe entregar hasta las **23hs del domingo 17 de agosto (hora de Argentina)**. La resolución del TP es **individual**. Pueden utilizar los contenidos vistos en clase y otra bibliografía. Si se toman ideas de fuentes externas deben ser correctamente citadas incluyendo el correspondiente link o página de libro.\n",
        "\n",
        "ESTE TP2 EQUIVALE AL 40% DE SU NOTA FINAL.\n",
        "\n",
        "El formato de entrega debe ser un link a un notebook de google colab. Permitir acceso a gvilcamiza.ext@fi.uba.ar y **habilitar los comentarios, para poder darles el feedback**. Si no lo hacen así no se podrá dar el feedback respectivo por cada pregunta.\n",
        "\n",
        "El envío **se realizará en el siguiente link de google forms: [link](https://forms.gle/wgKVpFhX6F6Nfkf46)**. Tanto los resultados, gráficas, como el código y las explicaciones deben quedar guardados y visualizables en el colab.\n",
        "\n",
        "**NO SE VALIDARÁN ENVÍOS POR CORREO, EL MÉTODO DE ENTREGA ES SOLO POR EL FORMS.**\n",
        "\n",
        "**Consideraciones a tener en cuenta:**\n",
        "- Se entregará 1 solo colab para este TP2.\n",
        "- Renombrar el archivo de la siguiente manera: **APELLIDO-NOMBRE-DL-TP2-Co20.ipynb**\n",
        "- Los códigos deben poder ejecutarse.\n",
        "- Los resultados, cómo el código, los gráficos y las explicaciones deben quedar guardados y visualizables en el correspondiente notebook.\n",
        "- Prestar atención a las consignas, responder las preguntas cuando corresponda.\n",
        "- Solo se revisarán los trabajos que hayan sido enviados por el forms."
      ],
      "metadata": {
        "id": "PEib4WVwfQYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLASIFICADOR DE EMOCIONES**"
      ],
      "metadata": {
        "id": "bdseNqG3m7xX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo de este trabajo es construir una red neuronal convolucional (CNN) utilizando Pytorch, capaz de clasificar emociones humanas a partir de imágenes faciales. El clasificador deberá identificar una de las 7 emociones básicas: alegría, tristeza, enojo, miedo, sorpresa, disgusto y seriedad. El dataset se encuentra en este link: https://drive.google.com/file/d/1pGsz3NFVHOfDpFG3jvpuM4NtpDCv757Z/view?usp=sharing"
      ],
      "metadata": {
        "id": "u8jyqDP8bom6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Preprocesamiento de Datos (2 puntos)\n",
        "\n",
        "Antes de entrenar el modelo, se debe analizar qué tipo de preprocesamiento se debe aplicar a las imágenes. Para esto, se puede considerar uno o más aspectos como:\n",
        "\n",
        "- Tamaño\n",
        "- Relación de aspecto\n",
        "- Color o escala de grises\n",
        "- Cambio de dimensionalidad\n",
        "- Normalización\n",
        "- Balanceo de datos\n",
        "- Data augmentation\n",
        "- etc.\n",
        "\n",
        "Sean criteriosos y elijan solo las técnicas que consideren pertinentes para este caso de uso en específico.\n",
        "\n",
        "Recomendación: usar `torchvision.transforms` para facilitar el preprocesamiento.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y-ouGrVnbp7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Construcción y entrenamiento del Modelo CNN (3.5 puntos)\n",
        "\n",
        "- Construir una red neuronal convolucional desde cero, sin usar modelos pre-entrenados.\n",
        "- Analizar correctamente qué funciones de activación se deben usar en cada etapa de la red, el learning rate a utilizar, la función de costo y el optimizador.\n",
        "- Cosas como el número de capas, neuronas, tanaño de kernel, entre otros, queda a criterio de ustedes, pero deben estar justificadas."
      ],
      "metadata": {
        "id": "Hk6B2VUvdufx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Evaluación del Modelo (2.5 puntos)\n",
        "\n",
        "El modelo entrenado debe ser evaluado utilizando las siguientes métricas:\n",
        "\n",
        "- **Accuracy**:\n",
        "  - Reportar el valor final en el conjunto de validación.\n",
        "  - Incluir una gráfica de evolución por época para entrenamiento y validación.\n",
        "\n",
        "- **F1 Score**:\n",
        "  - Reportar el valor final en el conjunto de validación.\n",
        "  - Incluir una gráfica de evolución por época para entrenamiento y validación.\n",
        "\n",
        "- **Costo (Loss)**:\n",
        "  - Mostrar una gráfica de evolución del costo por época para entrenamiento y validación.\n",
        "\n",
        "- **Classification report**\n",
        "  - Mostrar la precisión, recall y F1 score por cada clase usando `classification_report`\n",
        "\n",
        "- **Matriz de confusión**:\n",
        "  - Mostrar la matriz de confusión absoluta (valores enteros).\n",
        "  - Mostrar la matriz de confusión normalizada (valores entre 0 y 1 por fila).\n",
        "\n",
        "Se recomienda utilizar `scikit-learn` para calcular métricas como accuracy, F1 score, el Classification report y las matrices de confusión. Las visualizaciones pueden realizarse con `matplotlib` o `seaborn`, separando claramente los datos de entrenamiento y validación en las gráficas.\n"
      ],
      "metadata": {
        "id": "K5D3EvVRd-Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 4. Prueba de Imágenes Nuevas (1 punto)\n",
        "Subir al menos 5 imágenes personales de cualquier relación de aspecto (pueden usar fotos del rostro de ustedes, rostros de personas generadas por IA o imágenes stock de internet), que no formen parte del dataset de entrenamiento ni de validación.\n",
        "\n",
        "- Cada imagen debe representar una emoción distinta.\n",
        "\n",
        "- Aplicar el mismo pre-procesamiento que se usó para el dataset durante el entrenamiento del modelo.\n",
        "\n",
        "- Pasar las imágenes por el modelo entrenado y mostrar:\n",
        "\n",
        "  - La imagen original\n",
        "  - La imagen pre-procesada (mismas transformaciones del entrenamiento)\n",
        "  - El score asignado a cada clase\n",
        "  - La clase ganadora inferida por el modelo\n",
        "\n",
        "- Redactar conclusiones preliminares"
      ],
      "metadata": {
        "id": "40tsslqLgFlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## 5. Prueba de Imágenes Nuevas con Pre-procesamiento Adicional (1 punto)\n",
        "Las 5 imágenes del punto 4, ahora serán pasadas y recortadas por un algoritmo de detección de rostros. Usen el siguiente código para realizar un pre-procesamiento inicial de la imagen y ya luego aplican el pre-procesamiento que usaron al momento de entrenar el modelo.\n",
        "\n",
        "- Pasar las imágenes por el modelo entrenado y mostrar:\n",
        "  - La imagen original\n",
        "  - La imagen recortada por el algoritmo\n",
        "  - La imagen pre-procesada (mismas transformaciones del entrenamiento)\n",
        "  - El score asignado a cada clase\n",
        "  - La clase ganadora inferida por el modelo\n",
        "\n",
        "- Comparar los resultados con el punto 4 y redactar conclusiones finales.\n",
        "\n",
        "NOTA: Pueden adaptar el código según crean conveniente para obtener mejores resultados."
      ],
      "metadata": {
        "id": "w36xM5PrJ4GA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "image_path = \"\"\n",
        "\n",
        "image = cv2.imread(image_path)\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=6)\n",
        "\n",
        "image_with_box = image.copy()\n",
        "for (x, y, w, h) in faces:\n",
        "    cv2.rectangle(image_with_box, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "cropped_face_rgb = None\n",
        "if len(faces) > 0:\n",
        "    (x, y, w, h) = faces[0]\n",
        "    center_x, center_y = x + w // 2, y + h // 2\n",
        "    side = max(w, h)\n",
        "    half_side = side // 2\n",
        "\n",
        "    x1 = max(center_x - half_side, 0)\n",
        "    y1 = max(center_y - half_side, 0)\n",
        "    x2 = min(center_x + half_side, image.shape[1])\n",
        "    y2 = min(center_y + half_side, image.shape[0])\n",
        "\n",
        "    cropped_face = image[y1:y2, x1:x2]\n",
        "    cropped_face_rgb = cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "image_with_box_rgb = cv2.cvtColor(image_with_box, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "ax[0].imshow(image_with_box_rgb)\n",
        "ax[0].set_title(\"Detección\")\n",
        "ax[0].axis('off')\n",
        "\n",
        "if cropped_face_rgb is not None:\n",
        "    ax[1].imshow(cropped_face_rgb)\n",
        "    ax[1].set_title(\"Rostro recortado (relación aspecto 1:1)\")\n",
        "    ax[1].axis('off')\n",
        "else:\n",
        "    ax[1].text(0.5, 0.5, 'No se detectó rostro', horizontalalignment='center', verticalalignment='center')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "gBtAIm1KH2Qm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}